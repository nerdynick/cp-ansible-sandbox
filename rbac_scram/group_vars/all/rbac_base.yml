control_center_kafka_listener_name: rbac
schema_registry_kafka_listener_name: external #rbac
kafka_connect_kafka_listener_name: external #rbac

rbac_enabled: true

#START: These are custom variables used below. They are not part of CP-Ansible at this time
rbac_mds_public_key_filepath: pem/public.pem
rbac_mds_private_key_filepath: pem/tokenKeypair.pem
rbac_mds_public_key_filename: public.pem
rbac_mds_private_key_filename: tokenKeypair.pem
rbac_mds_public_key_path: "/var/ssl/private/{{rbac_mds_public_key_filename}}"
rbac_mds_private_key_path: "/var/ssl/private/{{rbac_mds_private_key_filename}}"
#END

kafka_broker_configure_additional_brokers: true #The following 3 sets are needed to provide proper advertisment
kafka_broker_inter_broker_listener_name: internal #Ensure we don't use the OAUTH/RBAC listener we enable
kafka_broker_custom_listeners: 
  rbac:
    name: RBAC
    port: 9093
    ssl_enabled: true
    ssl_mutual_auth_enabled: "{{ ssl_mutual_auth_enabled }}"
    sasl_protocol: OAUTHBEARER
    server_callback: io.confluent.kafka.server.plugins.auth.token.TokenBearerValidatorCallbackHandler
    login_callback: io.confluent.kafka.server.plugins.auth.token.TokenBearerServerLoginCallbackHandler
    options:
      publicKeyPath: "{{ rbac_mds_public_key_path }}"
    hostname: "{{inventory_hostname}}"
    
kafka_broker:
  properties:
    #MDS - RBAC Configurations
    authorizer.class.name: io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
    confluent.authorizer.access.rule.providers: ZK_ACL,CONFLUENT #Enable both Centralized ZK ACLs and RBAC
    #MDS - MDS & LDAP Configs
    super.users: User:admin;User:nikoleta-kfk1;User:nikoleta-kfk2;User:nikoleta
    broker.users: User:admin
    confluent.metadata.server.advertised.listeners: "http://{{inventory_hostname}}:8090,https://{{inventory_hostname}}:8091"
    confluent.metadata.server.listeners: http://0.0.0.0:8090,https://0.0.0.0:8091 #We do both a Plain & SSL here because Confluent CLI can't handle SSL right now fully or well
    confluent.metadata.server.authentication.method: BEARER
    confluent.metadata.server.public.key.path: "{{rbac_mds_public_key_path}}"
    confluent.metadata.server.token.key.path: "{{rbac_mds_private_key_path}}"
    confluent.metadata.server.ssl.keystore.location: "{{kafka_broker_keystore_path}}"
    confluent.metadata.server.ssl.keystore.password: "{{kafka_broker_keystore_storepass}}"
    confluent.metadata.server.ssl.key.password: "{{kafka_broker_keystore_keypass}}"
    confluent.metadata.server.ssl.truststore.location: "{{kafka_broker_truststore_path}}"
    confluent.metadata.server.ssl.truststore.password: "{{kafka_broker_truststore_storepass}}"
    confluent.metadata.server.token.max.lifetime.ms: 3600000
    #Sometimes you'll get the error "javax.naming.PartialResultException: Unprocessed Continuation Reference(s);" Changing the port can help.
    #  If you were using the port 389 change it to 3268
    #  If you were using the port 636 change it to 3269
    #  Reasoning: A GC (global catalog) server returns referrals on 389 to refer to the greater AD "forest", but acts like a regular LDAP server on 3268 (and 3269 for LDAPS)
    ldap.java.naming.provider.url: ldap://ad.bootcamp.confluent.io:3268
    ldap.java.naming.security.authentication: simple
    ldap.java.naming.security.credentials: "{{vault_service_account_password}}"
    ldap.java.naming.security.principal: "CN=Nikoleta KFK1,OU=Nikoleta,OU=BOOTCAMP,DC=ad,DC=bootcamp,DC=confluent,DC=io"
    #How to enable LDAPS (aka SSL/TLS for LDAP)
    #ldap.java.naming.security.protocol: SSL
    #ldap.ssl.truststore.location: "{{kafka_broker_truststore_path}}"
    #ldap.ssl.truststore.password: "{{kafka_broker_truststore_storepass}}"
    #Sample Group Based LDAP Search
    #ldap.group.name.attribute: sAMAccountName
    #ldap.group.object.class: group
    #ldap.group.search.base: OU=BOOTCAMP,DC=ad,DC=bootcamp,DC=confluent,DC=io
    #ldap.group.search.scope: 2
    #Sample User Based LDAP Search
    ldap.search.mode: USERS
    ldap.user.name.attribute: sAMAccountName
    ldap.user.object.class: user
    ldap.user.search.base: DC=ad,DC=bootcamp,DC=confluent,DC=io
    ldap.user.search.scope: 2
    ldap.user.memberof.attribute: memberOf
    ldap.user.memberof.attribute.pattern: "CN=(.*),OU=Nikoleta,OU=BOOTCAMP,DC=ad,DC=bootcamp,DC=confluent,DC=io"
    ldap.user.search.filter: "(memberOf=CN=NikoletaGroup,OU=Nikoleta,OU=BOOTCAMP,DC=ad,DC=bootcamp,DC=confluent,DC=io)"

control_center:
  properties:
    # Enable RBAC authorization in Control Center by providing a comma-separated list of Metadata Service (MDS) URLs
    confluent.metadata.bootstrap.server.urls: "{% for host in groups['kafka_broker'] %}{% if loop.index > 1%},{% endif %}http://{{ host }}:8090{% endfor %}"
    # MDS credentials of an RBAC user for Control Center to act on behalf of
    # NOTE: This user must be a SystemAdmin on each Apache Kafka cluster
    confluent.metadata.basic.auth.user.info: "nikoleta-kfk1:{{vault_service_account_password}}"
    # Enable authentication using a bearer token for Control Center's REST endpoints
    confluent.controlcenter.rest.authentication.method: BEARER
    # NOTE: Must match the MDS public key
    public.key.path: "{{rbac_mds_public_key_path}}"

schema_registry:
  properties:
    # These properties install the Schema Registry security plugin, and configure it to use |rbac| for
    # authorization and OAuth for authentication
    schema.registry.resource.extension.class: io.confluent.kafka.schemaregistry.security.SchemaRegistrySecurityResourceExtension
    confluent.schema.registry.authorizer.class: io.confluent.kafka.schemaregistry.security.authorizer.rbac.RbacAuthorizer
    rest.servlet.initializor.classes: io.confluent.common.security.jetty.initializer.InstallBearerOrBasicSecurityHandler
    confluent.schema.registry.auth.mechanism: JETTY_AUTH
    confluent.metadata.bootstrap.server.urls: "{% for host in groups['kafka_broker'] %}{% if loop.index > 1%},{% endif %}http://{{ host }}:8090{% endfor %}"
    # Credentials to use with the MDS, these should usually match those used for talking to Kafka
    confluent.metadata.basic.auth.user.info: "nikoleta-kfk1:{{vault_service_account_password}}"
    confluent.metadata.http.auth.credentials.provider: BASIC
    # The path to public keys that should be used to verify json web tokens during authentication
    public.key.path: "{{rbac_mds_public_key_path}}"
    # This enables anonymous access with a principal of User:ANONYMOUS
    confluent.schema.registry.anonymous.principal: "true"
    authentication.skip.paths: "/*"
    #This was needed for when not having a license key. However needs to be tested when one is provided
    kafkastore.connection.url: "{% for host in groups['zookeeper'] %}{% if loop.index > 1%},{% endif %}{{ host }}:{{zookeeper.properties.clientPort}}{% endfor %}"

kafka_connect:
  properties:
    # Adds the RBAC REST extension to the Connect worker
    rest.extension.classes: io.confluent.connect.security.ConnectSecurityExtension
    # The location of a running metadata service
    confluent.metadata.bootstrap.server.urls: "{% for host in groups['kafka_broker'] %}{% if loop.index > 1%},{% endif %}http://{{ host }}:8090{% endfor %}"
    # Credentials to use when communicating with the MDS
    confluent.metadata.basic.auth.user.info: "nikoleta-kfk1:{{vault_service_account_password}}"
    confluent.metadata.http.auth.credentials.provider: BASIC
    rest.servlet.initializor.classes: io.confluent.common.security.jetty.initializer.InstallBearerOrBasicSecurityHandler
    # The path to a directory containing public keys that should be used to verify json web tokens
    # during authentication
    public.key.path: "{{rbac_mds_public_key_path}}"
    #Allows override of producer/consumer settings via the JSON payload
    connector.client.config.override.policy: All
        